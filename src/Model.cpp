#include "Model.hpp"

using namespace std;

namespace STD {

Model::Model(const vector<Counts> &c, size_t T_, const Parameters &parameters_,
             bool same_coord_sys)
    : G(max_row_number(c)),
      T(T_),
      E(0),
      S(0),
      N(0),
      experiments(),
      parameters(parameters_),
      contributions_gene_type(Matrix::Zero(G, T)),
      contributions_gene(Vector::Zero(G)),
      phi_r(Matrix::Ones(G, T)),
      phi_p(Matrix::Ones(G, T)),
      mix_prior(sum_rows(c), T, parameters) {
  LOG(debug) << "Model G = " << G << " T = " << T << " E = " << E;
  size_t coord_sys = 0;
  for (auto &counts : c)
    add_experiment(counts, same_coord_sys ? 0 : coord_sys++);
  update_contributions();

  enforce_positive_parameters();

  initialize_coordinate_systems(1);
}

template <typename V>
vector<size_t> get_order(const V &v) {
  size_t N = v.size();
  vector<size_t> order(N);
  iota(begin(order), end(order), 0);
  sort(begin(order), end(order),
       [&v](size_t a, size_t b) { return v[a] > v[b]; });
  return order;
}

void Model::store(const string &prefix_, bool reorder) const {
  string prefix = parameters.output_directory + prefix_;
  auto factor_names = form_factor_names(T);
  auto &gene_names = experiments.begin()->counts.row_names;

  auto exp_gene_type = expected_gene_type();
  vector<size_t> order;
  if (reorder) {
    auto cs = colSums<Vector>(exp_gene_type);
    order = get_order(cs);
  }

#pragma omp parallel sections if (DO_PARALLEL)
  {
#pragma omp section
    write_matrix(exp_gene_type, prefix + "expected-features" + FILENAME_ENDING,
                 parameters.compression_mode, gene_names, factor_names, order);
#pragma omp section
    write_matrix(phi_r, prefix + "feature-gamma_prior-r" + FILENAME_ENDING,
                 parameters.compression_mode, gene_names, factor_names, order);
#pragma omp section
    write_matrix(phi_p, prefix + "feature-gamma_prior-p" + FILENAME_ENDING,
                 parameters.compression_mode, gene_names, factor_names, order);
#pragma omp section
    write_matrix(contributions_gene_type,
                 prefix + "contributions_gene_type" + FILENAME_ENDING,
                 parameters.compression_mode, gene_names, factor_names, order);
#pragma omp section
    write_vector(contributions_gene,
                 prefix + "contributions_gene" + FILENAME_ENDING,
                 parameters.compression_mode, gene_names);
#pragma omp section
    {
      auto print_field_matrix = [&](const string &path, const Vector &weights) {
        ofstream ofs(path);
        ofs << "coord_sys\tpoint_idx";
        for (size_t d = 0; d < coordinate_systems[0].mesh.dim; ++d)
          ofs << "\tx" << d;
        for (size_t t = 0; t < T; ++t)
          ofs << "\tFactor " << t + 1;
        ofs << endl;

        size_t coord_sys_idx = 0;
        for (size_t c = 0; c < coordinate_systems.size(); ++c) {
          for (size_t n = 0; n < coordinate_systems[c].N; ++n) {
            ofs << coord_sys_idx << "\t" << n;
            for (size_t d = 0; d < coordinate_systems[c].mesh.dim; ++d)
              ofs << "\t" << coordinate_systems[c].mesh.points[n][d];
            for (size_t t = 0; t < T; ++t)
              ofs << "\t"
                  << coordinate_systems[c].field(n, order[t])
                         * weights[order[t]];
            ofs << endl;
          }
          coord_sys_idx++;
        }
      };
      Vector weights = Vector::Ones(T);
      print_field_matrix(prefix + "field" + FILENAME_ENDING, weights);

      // NOTE we ignore local features and local baseline
      weights = mix_prior.r.array() / mix_prior.p.array();
      Vector meanColSums = colSums<Vector>(phi_r.array() / phi_p.array());
      for (size_t t = 0; t < T; ++t)
        weights(t) *= meanColSums(t);
      print_field_matrix(prefix + "expfield" + FILENAME_ENDING, weights);
    }
  }
  for (size_t e = 0; e < E; ++e) {
    string exp_prefix = prefix + "experiment"
                        + to_string_embedded(e, EXPERIMENT_NUM_DIGITS) + "-";
    experiments[e].store(exp_prefix, order);
  }
}

void Model::restore(const string &prefix) {
  contributions_gene_type = parse_file<Matrix>(
      prefix + "contributions_gene_type" + FILENAME_ENDING, read_matrix, "\t");
  contributions_gene
      = parse_file<Vector>(prefix + "contributions_gene" + FILENAME_ENDING,
                           read_vector<Vector>, "\t");
  phi_r = parse_file<Matrix>(prefix + "feature-gamma_prior-r" + FILENAME_ENDING,
                             read_matrix, "\t");
  phi_p = parse_file<Matrix>(prefix + "feature-gamma_prior-p" + FILENAME_ENDING,
                             read_matrix, "\t");
  for (size_t e = 0; e < E; ++e) {
    string exp_prefix = prefix + "experiment"
                        + to_string_embedded(e, EXPERIMENT_NUM_DIGITS) + "-";
    experiments[e].restore(exp_prefix);
  }
}

template <typename Fnc, typename Grad>
void optimize_newton_raphson(const string &tag, double &x, Fnc func,
                             Grad grad) {
  auto fnc = [&](double r) {
    double fn = func(r);
    double gr = grad(r);
    if (noisy)
      LOG(debug) << "local fnc/grad = " << fn << "/" << gr;
    return pair<double, double>(fn, gr);
  };

  if (noisy)
    LOG(debug) << "BEFORE: " << tag << "=" << x << " f=" << func(x);

  boost::uintmax_t it = NewtonRaphson::max_iter;
  double guess = x;
  x = boost::math::tools::newton_raphson_iterate(
      fnc, guess, NewtonRaphson::lower, guess * 100, NewtonRaphson::get_digits,
      it);
  if (it >= NewtonRaphson::max_iter) {
    LOG(fatal) << "Unable to locate solution for " << tag << " in "
               << NewtonRaphson::max_iter << " iterations.";
    LOG(fatal) << "prev=" << guess << " f(prev)=" << func(guess) << " cur=" << x
               << " f(cur)=" << func(x);

    if (abort_on_fatal_errors)
      exit(-1);
  }

  bool reached_upper = x >= NewtonRaphson::upper;

  if (noisy)
    LOG(debug) << "AFTER: " << tag << "=" << x << " f=" << func(x);

  if (reached_upper) {
    LOG(fatal) << "Error: reached upper limit for " << tag << "!";
    if (abort_on_fatal_errors)
      exit(-1);
  }
}

void Model::sample_local_r(size_t g, const vector<Matrix> counts_gst,
                           const Matrix &experiment_counts_gt,
                           const Matrix &experiment_theta_marginals,
                           mt19937 &rng) {
  const double a = parameters.hyperparameters.phi_r_1;
  const double b = parameters.hyperparameters.phi_r_2;
  for (size_t t = 0; t < T; ++t) {
    double A = a * 50;
    double B = b * 50;

    for (size_t e = 0; e < E; ++e) {
      double theta_marginal = experiment_theta_marginals(e, t)
                              * experiments[e].phi_b(g) * phi_r(g, t);

      if (experiment_counts_gt(e, t) == 0) {
        if (noisy)
          LOG(debug) << "Gibbs sampling local r of (" << g << ", " << t
                     << ") for experiment " << e << ": "
                     << experiments[e].counts.row_names[g];

        experiments[e].phi_l(g, t) = gamma_distribution<Float>(
            A, 1.0 / (B
                      - theta_marginal
                            * log(1 - neg_odds_to_prob(phi_p(g, t)))))(rng);

        if (noisy)
          LOG(debug) << "local r= " << experiments[e].phi_l(g, t);
      } else {
        if (noisy)
          LOG(debug) << "t = " << t << " experiment_counts_gt(e,t) = "
                     << experiment_counts_gt(e, t);
        auto fn0 = [&](double r) {
          if (noisy)
            LOG(debug) << "g/t/e = " << g << "/" << t << "/" << e << " r=" << r;

          const double no = phi_p(g, t);
          double fnc = theta_marginal * (log(no) - log(1 + no));
          for (size_t s = 0; s < experiments[e].S; ++s) {
            double prod = experiments[e].phi_b(g) * phi_r(g, t)
                          * experiments[e].theta(s, t) * experiments[e].spot(s);
            fnc += prod * digamma_diff(r * prod, counts_gst[e](s, t));
          }

          if (not parameters.ignore_priors)
            fnc += (A - 1) / r - B;

          return fnc;
        };

        auto gr0 = [&](double r) {
          double grad = 0;
          for (size_t s = 0; s < experiments[e].S; ++s) {
            double prod = experiments[e].phi_b(g) * phi_r(g, t)
                          * experiments[e].theta(s, t) * experiments[e].spot(s);
            double prod_sq = prod * prod;
            grad += prod_sq * trigamma_diff(r * prod, counts_gst[e](s, t));
          }

          if (not parameters.ignore_priors)
            grad += -(A - 1) / r / r;

          return grad;
        };

        optimize_newton_raphson("local r", experiments[e].phi_l(g, t), fn0,
                                gr0);
      }
    }
  }
}

void Model::sample_contributions(bool do_global_features,
                                 bool do_local_features, bool do_theta,
                                 bool do_baseline) {
  // for (auto &experiment : experiments)
  //  experiment.weights.matrix.ones();
  for (auto &experiment : experiments)
    experiment.spot.setOnes();

  const double a = parameters.hyperparameters.phi_r_1;
  const double b = parameters.hyperparameters.phi_r_2;
  const double alpha = parameters.hyperparameters.phi_p_1;
  const double beta = parameters.hyperparameters.phi_p_2;

  Matrix experiment_theta_marginals = Matrix::Zero(E, T);
  for (size_t e = 0; e < E; ++e)
    for (size_t s = 0; s < experiments[e].S; ++s)
      experiment_theta_marginals.row(e)
          += experiments[e].spot(s) * experiments[e].weights.matrix.row(s);

  do_local_features = do_local_features and E > 1;

  if (do_global_features or do_local_features or do_baseline)
#pragma omp parallel if (DO_PARALLEL)
  {
    auto rng = EntropySource::rngs[omp_get_thread_num()];

#pragma omp for
    for (size_t g = 0; g < G; ++g) {
      vector<Matrix> counts_gst;

      Matrix experiment_counts_gt = Matrix::Zero(E, T);
      for (size_t e = 0; e < E; ++e) {
        auto exp_counts = experiments[e].sample_contributions_gene(g, rng);
        experiment_counts_gt.row(e) = colSums<Vector>(exp_counts);
        counts_gst.push_back(exp_counts);
      }

      contributions_gene(g) = 0;
      for (size_t t = 0; t < T; ++t)
        contributions_gene_type(g, t) = 0;
      for (size_t e = 0; e < E; ++e)
        for (size_t t = 0; t < T; ++t)
          for (size_t s = 0; s < experiments[e].S; ++s) {
            contributions_gene(g) += counts_gst[e](s, t);
            contributions_gene_type(g, t) += counts_gst[e](s, t);
          }

      if (do_global_features and parameters.targeted(Target::global))
        for (size_t t = 0; t < T; ++t) {
          double cs = 0;
          for (size_t e = 0; e < E; ++e)
            cs += experiment_counts_gt(e, t);

          double theta_marginal = 0;
          for (size_t e = 0; e < E; ++e)
            theta_marginal += experiment_theta_marginals(e, t)
                              * experiments[e].phi_b(g)
                              * experiments[e].phi_l(g, t);

          auto r2p = [&](double r) {
            return (alpha + contributions_gene_type(g, t) - 1)
                   / (alpha + contributions_gene_type(g, t) + beta
                      + r * theta_marginal - 2);
          };
          auto r2no = [&](double r) {
            return (beta + r * theta_marginal - 1)
                   / (alpha + contributions_gene_type(g, t) - 1);
          };

          if (cs == 0) {
            if (noisy)
              LOG(debug) << "Gibbs sampling r and p of (" << g << ", " << t
                         << "): " << experiments[0].counts.row_names[g];

            phi_r(g, t) = gamma_distribution<Float>(
                a, 1.0 / (b
                          - theta_marginal
                                * log(1 - neg_odds_to_prob(phi_p(g, t)))))(rng);

            if (parameters.p_empty_map)
              // when this is used the r/p values lie along a curve, separated
              // from the MAP estimated ones
              phi_p(g, t) = r2no(phi_r(g, t));
            else
              // even when this is used the r/p values are somewhat separated
              // from the other values
              phi_p(g, t) = prob_to_neg_odds(sample_beta<Float>(
                  alpha, beta + phi_r(g, t) * theta_marginal, rng));

            if (noisy)
              LOG(debug) << "r/p= " << phi_r(g, t) << "/" << phi_p(g, t);
          } else {
            if (noisy)
              LOG(debug) << "t = " << t << " cs = " << cs;
            auto fn0 = [&](double r) {
              const double p = r2p(r);
              if (noisy)
                LOG(debug) << "g/t = " << g << "/" << t << " r=" << r
                           << " p=" << p;

              const double no = r2no(r);
              double fnc = theta_marginal * (log(no) - log(1 + no));
              for (size_t e = 0; e < E; ++e)
                for (size_t s = 0; s < experiments[e].S; ++s) {
                  double prod
                      = experiments[e].phi_b(g) * experiments[e].phi_l(g, t)
                        * experiments[e].theta(s, t) * experiments[e].spot(s);
                  fnc += prod * digamma_diff(r * prod, counts_gst[e](s, t));
                }

              if (not parameters.ignore_priors)
                fnc += (a - 1) / r - b;

              return fnc;
            };

            auto gr0 = [&](double r) {
              double grad = 0;
              for (size_t e = 0; e < E; ++e)
                for (size_t s = 0; s < experiments[e].S; ++s) {
                  double prod
                      = experiments[e].phi_b(g) * experiments[e].phi_l(g, t)
                        * experiments[e].theta(s, t) * experiments[e].spot(s);
                  double prod_sq = prod * prod;
                  grad
                      += prod_sq * trigamma_diff(r * prod, counts_gst[e](s, t));
                }

              if (not parameters.ignore_priors)
                grad += -(a - 1) / r / r;

              return grad;
            };

            optimize_newton_raphson("global feature r", phi_r(g, t), fn0, gr0);
            phi_p(g, t) = r2no(phi_r(g, t));
          }
        }

      // baseline feature

      if (do_baseline and parameters.targeted(Target::baseline)) {
        double A = a * 50;
        double B = b * 50;

        for (size_t e = 0; e < E; ++e) {
          vector<double> theta_marginals(T);
          double theta_marginal = 0;
          for (size_t t = 0; t < T; ++t) {
            theta_marginal += theta_marginals[t]
                = experiment_theta_marginals(e, t) * experiments[e].phi_l(g, t)
                  * phi_r(g, t);
          }

          auto fn0 = [&](double r) {
            if (noisy)
              LOG(debug) << "g/e = " << g << "/" << e << " r=" << r;

            double fnc = 0;
            for (size_t t = 0; t < T; ++t) {
              const double no = phi_p(g, t);
              fnc += theta_marginals[t] * (log(no) - log(1 + no));
              for (size_t s = 0; s < experiments[e].S; ++s) {
                double prod = experiments[e].phi_l(g, t) * phi_r(g, t)
                              * experiments[e].theta(s, t)
                              * experiments[e].spot(s);
                fnc += prod * digamma_diff(r * prod, counts_gst[e](s, t));
              }
            }

            if (not parameters.ignore_priors)
              fnc += (A - 1) / r - B;

            return fnc;
          };

          auto gr0 = [&](double r) {
            double grad = 0;
            for (size_t t = 0; t < T; ++t)
              for (size_t s = 0; s < experiments[e].S; ++s) {
                double prod = experiments[e].phi_l(g, t) * phi_r(g, t)
                              * experiments[e].theta(s, t)
                              * experiments[e].spot(s);
                double prod_sq = prod * prod;
                grad += prod_sq * trigamma_diff(r * prod, counts_gst[e](s, t));
              }

            if (not parameters.ignore_priors)
              grad += -(A - 1) / r / r;

            return grad;
          };

          optimize_newton_raphson("local feature baseline r",
                                  experiments[e].phi_b(g), fn0, gr0);
        }
      }

      // local features

      if (do_local_features and parameters.targeted(Target::local))
        sample_local_r(g, counts_gst, experiment_counts_gt,
                       experiment_theta_marginals, rng);
    }
  }
  update_contributions();

  // theta
  if (do_theta and parameters.targeted(Target::theta))
#pragma omp parallel if (DO_PARALLEL)
  {
    auto rng = EntropySource::rngs[omp_get_thread_num()];

    for (auto &experiment : experiments) {
#pragma omp for
      for (size_t s = 0; s < experiment.S; ++s) {
        auto counts_gst = experiment.sample_contributions_spot(s, rng);
        auto cs = colSums<Vector>(counts_gst);

        for (size_t t = 0; t < T; ++t)
          if (cs[t] == 0) {
            if (noisy)
              LOG(debug) << "Gibbs sampling theta(" << s << ", " << t << ").";
            double marginal = 0;
            for (size_t g = 0; g < G; ++g)
              marginal += experiment.phi_b(g) * phi_r(g, t)
                          * experiment.phi_l(g, t)
                          * log(1 - neg_odds_to_prob(phi_p(g, t)));
            marginal *= experiment.spot(s);
            experiment.theta(s, t) = gamma_distribution<Float>(
                mix_prior.r(t), 1.0 / (mix_prior.p(t) - marginal))(rng);
            if (noisy)
              LOG(debug) << "theta = " << experiment.theta(s, t);
          } else {
            auto fn0 = [&](double x) {
              double fnc = 0;
              for (size_t g = 0; g < G; ++g) {
                if (noisy)
                  LOG(debug) << "g/s/t = " << g << "/" << s << "/" << t
                             << " theta=" << x << " r=" << phi_r(g, t)
                             << " local r=" << experiment.phi_l(g, t)
                             << " p=" << neg_odds_to_prob(phi_p(g, t))
                             << " sigma=" << experiment.spot(s);

                // NOTE we don't multiply spot(s) in now, but once at the end
                double prod = experiment.phi_b(g) * phi_r(g, t)
                              * experiment.phi_l(g, t);

                fnc += prod * log(1 - neg_odds_to_prob(phi_p(g, t)));
                fnc += prod * digamma_diff(x * prod * experiment.spot(s),
                                           counts_gst(g, t));
              }

              fnc *= experiment.spot(s);

              if (not parameters.ignore_priors)
                // TODO ensure mix_prior.p is stored as negative odds
                fnc += (mix_prior.r(t) * experiment.field(s, t) - 1) / x
                       - mix_prior.p(t);

              return fnc;
            };

            auto gr0 = [&](double x) {
              double grad = 0;
              for (size_t g = 0; g < G; ++g) {
                // NOTE we don't multiply spot(s) in now, but once at the end
                double prod = experiment.phi_b(g) * phi_r(g, t)
                              * experiment.phi_l(g, t);
                double prod_sq = prod * prod;
                grad += prod_sq * trigamma_diff(x * prod * experiment.spot(s),
                                                counts_gst(g, t));
              }

              grad *= experiment.spot(s) * experiment.spot(s);

              if (not parameters.ignore_priors)
                // NOTE TODO this needs testing! the minus sign was missing
                grad += -(mix_prior.r(t) * experiment.field(s, t) - 1) / x / x;

              return grad;
            };

            optimize_newton_raphson("theta", experiment.theta(s, t), fn0, gr0);
          }
      }
    }
  }
  update_contributions();

  enforce_positive_parameters();

  if (parameters.targeted(Target::theta_prior))
    sample_global_theta_priors();

  enforce_positive_parameters();

  if (parameters.targeted(Target::field))
    update_fields();

  enforce_positive_parameters();
}

Matrix Model::field_fitness_posterior_gradient(const Matrix &f) const {
  Matrix grad(S, T);
  size_t cumul = 0;
  for (auto &experiment : experiments) {
    grad.middleRows(cumul, experiment.S)
        = experiment.field_fitness_posterior_gradient(
            f.middleRows(cumul, experiment.S));
    cumul += experiment.S;
  }
  return grad;
}

void Model::update_fields() {
  LOG(verbose) << "Updating fields";

  using namespace LBFGSpp;
  LBFGSParam<double> param;
  param.epsilon = parameters.lbfgs_epsilon;
  param.max_iterations = parameters.lbfgs_iter;
  // Create solver and function object
  LBFGSSolver<double> solver(param);

  for (auto &coord_sys : coordinate_systems) {
    LOG(verbose) << "Updating coordinate system";
    using Vec = Eigen::VectorXd;
    const size_t NT = coord_sys.N * T;
    Vec x(NT);

    LOG(debug) << "initial field: " << endl << Stats::summary(coord_sys.field);

    for (size_t i = 0; i < NT; ++i)
      // NOTE log for grad w.r.t. exp-transform
      x(i) = log(coord_sys.field(i));

    size_t call_cnt = 0;

    auto fnc = [&](const Vec &field_, Vec &grad_) {
      Matrix field(coord_sys.N, T);
      for (size_t i = 0; i < NT; ++i)
        field(i) = exp(field_(i));
      Matrix grad;
      double score = field_gradient(coord_sys, field, grad);
      for (size_t i = 0; i < NT; ++i)
        // NOTE multiply w field to compute gradient of exp-transform
        grad_(i) = grad(i) * field(i);
      if (((call_cnt++) % parameters.lbfgs_report_interval) == 0) {
        LOG(debug) << "Field score = " << score;
        LOG(debug) << "field: " << endl << Stats::summary(field);
        LOG(debug) << "grad: " << endl << Stats::summary(grad);
        Matrix tmp = grad * field;
        LOG(debug) << "grad*field: " << endl << Stats::summary(tmp);
      }
      return score;
    };

    double fx;
    int niter = solver.minimize(fnc, x, fx);

    for (size_t i = 0; i < NT; ++i)
      coord_sys.field(i) = exp(x(i));

    LOG(verbose) << "LBFGS performed " << niter << " iterations";
    LOG(verbose) << "LBFGS evaluated function and gradient " << call_cnt
                 << " times";
    LOG(verbose) << "LBFGS achieved f(x) = " << fx;
    LOG(verbose) << "LBFGS field summary: " << endl
                 << Stats::summary(coord_sys.field);
  }
  update_experiment_fields();
}

double Model::field_gradient(CoordinateSystem &coord_sys, const Matrix &field,
                             Matrix &grad) const {
  LOG(debug) << "field dim " << field.rows() << "x" << field.cols();
  double score = 0;

  Matrix fitness(S, T);
  size_t cumul = 0;
  for (auto member : coord_sys.members) {
    double s = -experiments[member]
                    .field_fitness_posterior(
                        field.middleRows(cumul, experiments[member].S))
                    .sum();
    LOG(debug) << "Fitness contribution to score of sample " << member << ": "
               << s;
    score += s;
    fitness.middleRows(cumul, experiments[member].S)
        = experiments[member].field_fitness_posterior_gradient(
            field.middleRows(cumul, experiments[member].S));
    cumul += experiments[member].S;
  }

  LOG(debug) << "Fitness contribution to score: " << score;

  grad = Matrix::Zero(coord_sys.N, T);

  grad.topRows(coord_sys.S) = -fitness;

  if (parameters.field_lambda_dirichlet != 0) {
    Matrix grad_dirichlet = Matrix::Zero(coord_sys.N, T);
    for (size_t t = 0; t < T; ++t) {
      grad_dirichlet.col(t)
          = coord_sys.mesh.grad_dirichlet_energy(Vector(field.col(t)));

      double s = parameters.field_lambda_dirichlet
                 * coord_sys.mesh.sum_dirichlet_energy(Vector(field.col(t)));
      score += s;
      LOG(debug) << "Smoothness contribution to score of factor " << t << ": "
                 << s;
    }
    grad = grad + grad_dirichlet * parameters.field_lambda_dirichlet;
  }

  if (parameters.field_lambda_laplace != 0) {
    Matrix grad_laplace = Matrix::Zero(coord_sys.N, T);
    for (size_t t = 0; t < T; ++t) {
      grad_laplace.col(t)
          = coord_sys.mesh.grad_sq_laplace_operator(Vector(field.col(t)));

      double s = parameters.field_lambda_laplace
                 * coord_sys.mesh.sum_sq_laplace_operator(Vector(field.col(t)));
      score += s;
      LOG(debug) << "Curvature contribution to score of factor " << t << ": "
                 << s;
    }
    grad = grad + grad_laplace * parameters.field_lambda_laplace;
  }

  LOG(debug) << "Fitness and smoothness score: " << score;

  return score;
}

void Model::enforce_positive_parameters() {
  for (auto &coord_sys : coordinate_systems)
    enforce_positive_and_warn("field", coord_sys.field);
  enforce_positive_and_warn("phi_r", phi_r);
  enforce_positive_and_warn("phi_p", phi_p);
  for (auto &experiment : experiments)
    experiment.enforce_positive_parameters();
}

void Model::sample_global_theta_priors() {
  // TODO refactor
  Matrix observed(S, T);
  size_t cumul = 0;
  for (auto &experiment : experiments) {
    observed.middleRows(cumul, experiment.S) = experiment.weights.matrix;
    cumul += experiment.S;
  }

  if (parameters.normalize_spot_stats) {
    auto rs = rowSums<Vector>(observed);
    for (size_t t = 0; t < T; ++t)
      observed.col(t).array() /= rs.array();
  }

  Matrix field(S, T);
  cumul = 0;
  for (auto &experiment : experiments) {
    field.middleRows(cumul, experiment.S) = experiment.field;
    cumul += experiment.S;
  }

  mix_prior.sample(observed, field);

  for (auto &experiment : experiments)
    experiment.weights.prior = mix_prior;

  min_max("weights r", mix_prior.r);
  min_max("weights p", mix_prior.p);
}

double Model::log_likelihood(const string &prefix) const {
  double l = 0;
  for (size_t e = 0; e < E; ++e) {
    Matrix m = experiments[e].log_likelihood();

    auto &gene_names = experiments[e].counts.row_names;
    auto &spot_names = experiments[e].counts.col_names;

    string exp_prefix = prefix + "experiment"
                        + to_string_embedded(e, EXPERIMENT_NUM_DIGITS) + "-";
    write_matrix(m, exp_prefix + "loglikelihood" + FILENAME_ENDING,
                 parameters.compression_mode, gene_names, spot_names);
    write_matrix(*experiments[e].counts.matrix,
                 exp_prefix + "loglikelihood-counts" + FILENAME_ENDING,
                 parameters.compression_mode, gene_names, spot_names);
    for (auto &x : m)
      l += x;
  }
  return l;
}

// computes a matrix M(g,t)
// with M(g,t) = phi(g,t) sum_e local_baseline_phi(e,g) local_phi(e,g,t) sum_s
// theta(e,s,t) sigma(e,s)
Matrix Model::expected_gene_type() const {
  Matrix m = Matrix::Zero(G, T);
  for (auto &experiment : experiments)
    m += experiment.expected_gene_type();
  return m;
}

void Model::update_experiment_fields() {
  LOG(verbose) << "Updating experiment fields";
  for (auto &coord_sys : coordinate_systems) {
    size_t cumul = 0;
    for (auto member : coord_sys.members) {
      experiments[member].field
          = coord_sys.field.middleRows(cumul, experiments[member].S);
      cumul += experiments[member].S;
    }
  }
}

void Model::update_contributions() {
  contributions_gene_type.setZero();
  contributions_gene.setZero();
  for (auto &experiment : experiments)
#pragma omp parallel for if (DO_PARALLEL)
    for (size_t g = 0; g < G; ++g) {
      contributions_gene(g) += experiment.contributions_gene(g);
      for (size_t t = 0; t < T; ++t)
        contributions_gene_type(g, t)
            += experiment.contributions_gene_type(g, t);
    }
}

void Model::initialize_coordinate_systems(double v) {
  size_t coord_sys_idx = 0;
  for (auto &coord_sys : coordinate_systems) {
    size_t num_additional = parameters.mesh_additional;
    coord_sys.S = 0;
    for (auto &member : coord_sys.members)
      coord_sys.S += experiments[member].S;
    if (coord_sys.S == 0)
      num_additional = 0;
    coord_sys.N = coord_sys.S + num_additional;
    coord_sys.T = T;
    coord_sys.field = Matrix(coord_sys.N, T);
    coord_sys.field.fill(v);

    using Point = Vector;
    size_t dim = experiments[coord_sys.members[0]].coords.cols();
    vector<Point> pts;
    {
      Point pt(dim);
      for (auto &member : coord_sys.members)
        for (size_t s = 0; s < experiments[member].S; ++s) {
          for (size_t i = 0; i < dim; ++i)
            pt[i] = experiments[member].coords(s, i);
          pts.push_back(pt);
        }

      if (num_additional > 0) {
        Point mi = pts[0];
        Point ma = pts[0];
        for (auto &p : pts)
          for (size_t d = 0; d < dim; ++d) {
            if (p[d] < mi[d])
              mi[d] = p[d];
            if (p[d] > ma[d])
              ma[d] = p[d];
          }
        if (parameters.mesh_hull_distance <= 0) {
          Point mid = (ma + mi) / 2;
          Point half_diff = (ma - mi) / 2;
          mi = mid - half_diff * parameters.mesh_hull_enlarge;
          ma = mid + half_diff * parameters.mesh_hull_enlarge;
        } else {
          mi.array() -= parameters.mesh_hull_distance;
          ma.array() += parameters.mesh_hull_distance;
        }
        for (size_t i = 0; i < num_additional; ++i) {
          // TODO improve this
          // e.g. enlarge area, use convex hull instead of bounding box, etc
          bool ok = false;
          while (not ok) {
            for (size_t d = 0; d < dim; ++d)
              pt[d] = mi[d]
                      + (ma[d] - mi[d])
                            * RandomDistribution::Uniform(EntropySource::rng);
            for (size_t s = 0; s < coord_sys.S; ++s)
              if ((pt - pts[s]).norm() < parameters.mesh_hull_distance) {
                ok = true;
                break;
              }
          }

          pts.push_back(pt);
        }
      }
    }
    coord_sys.mesh
        = Mesh(dim, pts,
               parameters.output_directory + "coordsys"
                   + to_string_embedded(coord_sys_idx, EXPERIMENT_NUM_DIGITS));
    coord_sys_idx++;
    S += coord_sys.S;
    N += coord_sys.N;
  }
}

void Model::add_experiment(const Counts &counts, size_t coord_sys) {
  Parameters experiment_parameters = parameters;
  const double factor = parameters.local_phi_scaling_factor;
  experiment_parameters.hyperparameters.phi_p_1 *= factor;
  experiment_parameters.hyperparameters.phi_r_1 *= factor;
  experiment_parameters.hyperparameters.phi_p_2 *= factor;
  experiment_parameters.hyperparameters.phi_r_2 *= factor;
  experiments.push_back({this, counts, T, experiment_parameters});
  E++;
  while (coordinate_systems.size() <= coord_sys)
    coordinate_systems.push_back({});
  coordinate_systems[coord_sys].members.push_back(E - 1);
}

ostream &operator<<(ostream &os, const Model &model) {
  os << "Spatial Transcriptome Deconvoltuion "
     << "G = " << model.G << " "
     << "T = " << model.T << " "
     << "E = " << model.E << endl;

  /*
  if (verbosity >= Verbosity::debug) {
    print_matrix_head(os, model.features.matrix, "Φ");
    os << model.phi;
  }
  */
  for (auto &experiment : model.experiments)
    os << experiment;

  return os;
}

Model operator*(const Model &a, const Model &b) {
  Model model = a;

  model.contributions_gene_type.array() *= b.contributions_gene_type.array();
  model.contributions_gene.array() *= b.contributions_gene.array();
  model.phi_r.array() *= b.phi_r.array();
  model.phi_p.array() *= b.phi_p.array();
  for (size_t e = 0; e < model.E; ++e)
    model.experiments[e] = model.experiments[e] * b.experiments[e];

  return model;
}

Model operator+(const Model &a, const Model &b) {
  Model model = a;

  model.contributions_gene_type += b.contributions_gene_type;
  model.contributions_gene += b.contributions_gene;
  model.phi_r += b.phi_r;
  model.phi_p += b.phi_p;
  for (size_t e = 0; e < model.E; ++e)
    model.experiments[e] = model.experiments[e] + b.experiments[e];

  return model;
}

Model operator-(const Model &a, const Model &b) {
  Model model = a;

  model.contributions_gene_type -= b.contributions_gene_type;
  model.contributions_gene -= b.contributions_gene;
  model.phi_r -= b.phi_r;
  model.phi_p -= b.phi_p;
  for (size_t e = 0; e < model.E; ++e)
    model.experiments[e] = model.experiments[e] - b.experiments[e];

  return model;
}

Model operator*(const Model &a, double x) {
  Model model = a;

  model.contributions_gene_type *= x;
  model.contributions_gene *= x;
  model.phi_r *= x;
  model.phi_p *= x;
  for (auto &experiment : model.experiments)
    experiment = experiment * x;

  return model;
}

Model operator/(const Model &a, double x) {
  Model model = a;

  model.contributions_gene_type /= x;
  model.contributions_gene /= x;
  model.phi_r /= x;
  model.phi_p /= x;
  for (auto &experiment : model.experiments)
    experiment = experiment / x;

  return model;
}
}
