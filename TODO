* loading of models from files
* compute covariance of spots across cell type contributions during MCMC and accumulate statistics -> clustering of spots
* perform DE between sets of spots clustered according to MCMC-accumulated covariance and based on uncertainty propagated to spot-level
* improve code for and CLI to freeze the scaling factors
* compute means etc. on chain statistics:
  - point estimates: mean, standard deviation, percentiles
  - interval estimates: equal-tail and highest posterior density (HPD) intervals
  - posterior covariance matrix
  - posterior correlation matrix
  - deviance information criterion (DIC)
* determine and direct acceptance rate of Metropolis-Hastings sampling
* write out predicted means and variances, based on
  - the NB conditional posterior
  - the NM conditional posterior
  - the Poisson conditional posterior
* re-consider split and merge algorithm
* for the splitting / merging:
  - independently also optimize the parameters without splitting / merging
* add flag for consistent contribution marginals and expectation marginals
* contribution initialization is unneeded if Gibbs sampling does that as a first step
* experiment with splitting a single factor explaining everything
* compute perplexity, i.e. validation
* automatically determine single experiments in analyze.r
* store all iterations' parameters;
* compute auto-correlation
* print out acceptance / rejection statistics for MH parts
* test likelihood optimization using ML
* figure out what kind of splitting / merging proposal distribution is currently realized
* consider hierarchies of factors
* enable proper before / after printing for merging and splitting
* trans-experiment factors:
  - by using data in blocks corresponding to the experiments
  - alternative: make sense of trans-experiment profiles post-analysis
* experiment-wise mix-in of factors
  - like normal factors (with features and weights in the same matrices), but treated differently when sampling theta
* use faster method for sampling from the Poisson distribution
* bring back statistical summary printing for vectors
* templatize scaling variables
* evaluate log-normal poisson rate modeling
* differential expression analysis
* check the Dirichlet posteriors
  - done! the feature side is as in that publication; the mixing side is different but ultimately analogous; I haven't written this down yet
* analysis script:
  - create barplots for expected reads explained, aside from expected spots explained;
  - also break-down experiment-wise
  - aside from basing on theta, also consider using the assigned contributions
    o do this across iterations, for each in decreasing order (because of non-identifiability due to exchangability!), coloring the first say in red, the last in black to see whether (and if so, how long) there change in the distribution

implement t1, t2, g1, g2, s1, s2 to set limits for sampling

* experiment-constant factors?
* what would be simpler than this is factors that are zero in the other samples, otherwise free

hierarchical dirichlet features
a two-level hierarchy

when sampling a novel type (i.e. one that isn't already present in the global hierarchy) then sample a dirichlet-multinomial as dirichlet prior for it, with a similar scaling as the initial factors
currently, the nHDP results in trees with a lot of weight on a single branch; it would be good to get more balanced trees instead!
it would be possible to sample multionomially for all counts of a gene in a spot
Kmeans: use a gaussian mixture approach, rather than 'hard' K-means

do bagged updates
consider re-setting the mixing counts from time to time

consider performing a dirichlet process in which the observations are count vectors
implement the HDP

drop template-based approach in favor of inheritance

* is it possible to benefit even more from sparcity? consider non-zero values; this might make various things faster

less important:
* collect log-likelihoods and write to file
* note that some sums across factors, genes, and / or spots may yield numbers greater than 2^16; however the count data type uses 32 bit! So this should be unproblematic
* use something like libmagic to figure out file types
* consider using GPU for sampling contributions
* experiment with activating ARMA_NO_DEBUG in types.hpp
* write probabilistic unit tests for sampling routines
* consider using HDF5 for IO
* test possible benefits of JIT compiling with fixed G, S, T
  - the speed-up was marginal in a manual test it went from 2:05 min to 2:00 (round about...). unfortunately I forgot whether I used all or only the top 1000 genes; some speed-ups only become apparent with more parameters!
* use profile-generate / profile-use gcc flags
* enforcing medians as alternative to means

* print out some sort of Bayesian summary
 -> percentiles: 0%, 5%, 25%, 50%, 75%, 95%, 100%
 -> mean and variance

if gamma mixture proportions are used then the spot scaling should probably be deactivated

feature-side hierarchy:
basically, introduce a plate for gene, type, and experiment.
Let it's structure be identical to that of the gene and type plate.
I.e. then we have \lambda_{gst} = \phi_{gt} \phi_{gte} \theta{st} \sigma{s} maybe times \sigma{e}

Terminology: "groups" are what I formerly called "experiments".

It might be nice to turn the experiment-specific hyper-parameters into random variables
This might be feasible because we have so many parameters sharing that parameter.
Exponential distributions could be used, and estimation be performed by MH.

It might also be nice to have the same kind of hierarchical structure on the mixing side
Like that it would be possible for the theta priors to share information across groups.

The same generalized beta prime / compound gamma distributions could be used to sample the 2nd prior of the mixing weights as is used for the 2nd prior of the mixing components.

Ressurect:
 * experiment-specific phi prior estimation
 * experiment scaling

Determine how similar the "real" local gene expression profiles (the component-wise product of local and global phi matrices) really are to the global ones.
Consider using such a similarity measure to determine homogeneity amongst the "real" local profiles, identifying outliers and associating them with other global profiles.

Fix the likelihood calculation of phi's p prior

Write DGE analysis code
 * compare all factors with each other within each experiment
 * requires code to compute the distribution
 * the question of the baselines arises
   - several approaches exist:
     > arg min sum log FC
     > arg min Jensen-Shannon divergence

Verify the implementation of the experiment-dependent baseline feature
i.e.: lamba_gst = phi_gt global_phi_gt experiment_phi_g theta_st sigma_s

Re-implent dirichlet features and mixing weights

Modularize calling modes:
 * only global
 * only local
 * both
In each case, for the local features, we are NOT learning the priors

Study mean vs CV plots

Make sure the subroutine call in Metropolis Hastings for the prior of theta is inlined!

Simplify template boiler plate code by contracting; i.e. form a template struct that carries the combined information of the two current template arguments, and expect the model / experiment etc template types to be instantiated with bespoke template struct.

It might be good to test for each effective local factor, whether or not it would fit better to other global factors

add a baseline mixing weight factor?
then, what about global mixing weights?

ensure that global_theta_prior is off when using dirichlet mixtures; similarly actually for all theta priors

Add a sampling mode for the end, in which the theta values are randomized, and then contributions and theta values are updated iteratively.
This would be done multiple times, and the resulting theta matrix reported.
Importantly, in doing so, everything feature-side, as well as the spot scaling variables, and global theta priors should not be touched.
When local theta priors are used, then it's debatable whether those should be fixed or also randomized & updated.

Global baseline feature
Feature hierarchy: tree

Baseline mixture
