* storing loading of models to and from files
* compute covariance of spots across cell type contributions during MCMC and accumulate statistics -> clustering of spots
* perform DE between sets of spots clustered according to MCMC-accumulated covariance and based on uncertainty propagated to spot-level
* improve code for and CLI to freeze the scaling factors
* compute means etc. on chain statistics:
  - point estimates: mean, standard deviation, percentiles
  - interval estimates: equal-tail and highest posterior density (HPD) intervals
  - posterior covariance matrix
  - posterior correlation matrix
  - deviance information criterion (DIC)
* determine and direct acceptance rate of Metropolis-Hastings sampling
* write out predicted means and variances, based on
  - the NB conditional posterior
  - the NM conditional posterior
  - the Poisson conditional posterior
* write out for the split and merge steps before splitting / merging and those afterwards:
  - the sub-counts
  - the normalized theta matrices (and the other parameters, too!)
* for the splitting / merging:
  - independently also optimize the parameters without splitting / merging
* add flag for consistent contribution marginals and expectation marginals
* contribution initialization is unneeded if Gibbs sampling does that as a first step
* experiment with splitting a single factor explaining everything
* don't split/merge in initial iterations
* compute perplexity, i.e. validation
* automatically determine single experiments in analyze.r
* use sub model lifting to initialize sub models for inspection of before / after effects of splitting and merging
* store all iterations' parameters;
* compute auto-correlation
* print out acceptance / rejection statistics for MH parts
* test likelihood optimization using ML
* figure out what kind of splitting / merging proposal distribution is currently realized
* consider hierarchies of factors
* enable proper before / after printing for merging and splitting
* trans-experiment factors:
  - by using data in blocks corresponding to the experiments
  - alternative: make sense of trans-experiment profiles post-analysis
* alternative to split / merge:
  - multiple re-starts
* experiment-wise mix-in of factors
  - like normal factors (with features and weights in the same matrices), but treated differently when sampling theta
* use faster method for sampling from the Poisson distribution
* bring back statistical summary printing for vectors
* templatize scaling variables
* evaluate log-normal poisson rate modeling
* differential expression analysis
* check the Dirichlet posteriors
  - done! the feature side is as in that publication; the mixing side is different but ultimately analogous; I haven't written this down yet
* analysis script:
  - create barplots for expected reads explained, aside from expected spots explained;
  - also break-down experiment-wise
  - aside from basing on theta, also consider using the assigned contributions
    o do this across iterations, for each in decreasing order (because of non-identifiability due to exchangability!), coloring the first say in red, the last in black to see whether (and if so, how long) there change in the distribution

implement t1, t2, g1, g2, s1, s2 to set limits for sampling

experiment-constant factors:
* improve experiment handling
* needs modification:
  - PartialModel<Mix>::sample()
  - Model::Model()
  - Model::split()
  - Model::merge()

* what would be simpler than this is factors that are zero in the other samples, otherwise free

hierarchical dirichlet features
a two-level hierarchy

when sampling a novel type (i.e. one that isn't already present in the global hierarchy) then sample a dirichlet-multinomial as dirichlet prior for it, with a similar scaling as the initial factors
currently, the nHDP results in trees with a lot of weight on a single branch; it would be good to get more balanced trees instead!
it would be possible to sample multionomially for all counts of a gene in a spot
Kmeans: use a gaussian mixture approach, rather than 'hard' K-means

do bagged updates

* is it possible to benefit even more from sparcity? consider non-zero values; this might make various things faster

less important:
* collect log-likelihoods and write to file
* note that some sums across factors, genes, and / or spots may yield numbers greater than 2^16; however the count data type uses 32 bit! So this should be unproblematic
* use something like libmagic to figure out file types
* consider using GPU for sampling contributions
* experiment with activating ARMA_NO_DEBUG in types.hpp
* write probabilistic unit tests for sampling routines
* consider using HDF5 for IO
* test possible benefits of JIT compiling with fixed G, S, T
  - the speed-up was marginal in a manual test it went from 2:05 min to 2:00 (round about...). unfortunately I forgot whether I used all or only the top 1000 genes; some speed-ups only become apparent with more parameters!
* use profile-generate / profile-use gcc flags
* enforcing medians as alternative to means

* print out some sort of Bayesian summary
